{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jeopardy is a popular TV show in the US where participants answer questions to win money. It's been running for many years, and is a major force in popular culture. If you need help at any point, you can consult our solution notebook here.\n",
    "\n",
    "Imagine that you want to compete on Jeopardy, and you're looking for any way to win. In this project, you'll work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help you win.\n",
    "\n",
    "The dataset is named jeopardy.csv, and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which you can download here. Here's the beginning of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy = pd.read_csv('jeopardy.csv')\n",
    "\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each row in the dataset represents a single question on a single episode of Jeopardy. Here are explanations of each column:\n",
    "\n",
    "- Show Number - the Jeopardy episode number\n",
    "- Air Date - the date the episode aired\n",
    "- Round - the round of Jeopardy\n",
    "- Category - the category of the question\n",
    "- Value - the number of dollars the correct answer is worth\n",
    "- Question - the text of the question\n",
    "- Answer - the text of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing blank spaces\n",
    "jeopardy.columns = ['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question', 'Answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing text\n",
    "\n",
    "Before you can start doing analysis on the Jeopardy questions, you need to normalize all of the text columns (the Question and Answer columns). We covered normalization before, but the idea is to ensure that you put words in lowercase and remove punctuation so Don't and don't aren't considered to be different words when you compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub('[^\\w\\s]', '', s)\n",
    "    return s\n",
    "\n",
    "a = 'Hol^\"A'\n",
    "normalize_text(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the `Question` and `Answer` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['clean_question'] = jeopardy['Question'].apply(normalize_text)\n",
    "jeopardy['clean_answer'] = jeopardy['Answer'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing columns\n",
    "\n",
    "Now that you've normalized the text columns, there are also some other columns to normalize.\n",
    "\n",
    "The `Value` column should be numeric, to allow you to manipulate it easier. You'll need to remove the dollar sign from the beginning of each value and convert the column from text to numeric.\n",
    "\n",
    "The `Air Date` column should also be a datetime, not a string, to enable you to work it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dollars(s):\n",
    "    try:\n",
    "        return int(re.sub('[^\\w\\s]', '', s))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normalize_dollars)\n",
    "jeopardy['Air Date'] = pd.to_datetime(jeopardy['Air Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers in questions\n",
    "\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "- How often the answer can be used for a question.\n",
    "- How often questions are repeated.\n",
    "\n",
    "You can answer the second question by seeing **how often complex words (> 6 characters) reoccur**. \n",
    "You can answer the first question by seeing **how many times words in the answer also occur in the question**. We'll work on the first question and come back to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(row): \n",
    "    \"\"\"\n",
    "    Counts how many times words in 'clean_answer' appear in 'clean_question'. The input is a pd.Series (a row in the dataset)\n",
    "    \"\"\"\n",
    "    split_answer = row['clean_answer'].split(' ')\n",
    "    split_question = row['clean_question'].split(' ')\n",
    "    \n",
    "    # removing 'the'\n",
    "    if 'The' in split_answer:\n",
    "        split_answer.remove('The')\n",
    "    elif 'the' in split_answer:\n",
    "        split_answer.remove('the')\n",
    "    \n",
    "    # Edge case\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Matches\n",
    "    match_count = len( set(split_answer).intersection(set(split_question)) )\n",
    "    \n",
    "    return match_count / len(split_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['answer_in_question'] = jeopardy.apply(count_matches, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    86.879344\n",
       "0.500000     7.235362\n",
       "0.333333     2.770139\n",
       "0.250000     0.845042\n",
       "1.000000     0.580029\n",
       "0.666667     0.515026\n",
       "0.200000     0.435022\n",
       "0.166667     0.155008\n",
       "0.142857     0.130007\n",
       "0.400000     0.120006\n",
       "0.750000     0.090005\n",
       "0.125000     0.055003\n",
       "0.285714     0.035002\n",
       "0.600000     0.035002\n",
       "0.800000     0.015001\n",
       "0.428571     0.010001\n",
       "0.714286     0.010001\n",
       "0.875000     0.010001\n",
       "0.100000     0.010001\n",
       "0.181818     0.010001\n",
       "0.222222     0.010001\n",
       "0.111111     0.010001\n",
       "0.300000     0.010001\n",
       "0.571429     0.005000\n",
       "0.307692     0.005000\n",
       "0.153846     0.005000\n",
       "0.375000     0.005000\n",
       "0.272727     0.005000\n",
       "Name: answer_in_question, dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['answer_in_question'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we find out that in some cases words that appear in the question are also present in the correct answer. However, it is actually just a minority of the total questions. It is probably not worthy base the answer in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recycled questions\n",
    "\n",
    "Let's say you want to investigate how often new questions are repeats of older ones. You can't completely answer this, because you only have about 10% of the full Jeopardy question dataset, but you can investigate it at least.\n",
    "\n",
    "To do this, you can:\n",
    "\n",
    "- Sort `jeopardy` in order of ascending air date.\n",
    "- Maintain a set called `terms_used` that will be empty initially.\n",
    "- Iterate through each row of `jeopardy`.\n",
    "- Split `clean_question` into words, remove any word shorter than `6` characters, and check if each word occurs in `terms_used`.\n",
    "    - If it does, increment a counter.\n",
    "    - Add each word to `terms_used`.\n",
    "     \n",
    "This allows you to check if the terms in questions have been used previously or not. Only looking at words with six or more characters enables you to filter out words like `the` and `than`, which are commonly used, but don't tell you a lot about a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_answer</th>\n",
       "      <th>clean_value</th>\n",
       "      <th>answer_in_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19325</th>\n",
       "      <td>10</td>\n",
       "      <td>1984-09-21</td>\n",
       "      <td>Final Jeopardy!</td>\n",
       "      <td>U.S. PRESIDENTS</td>\n",
       "      <td>None</td>\n",
       "      <td>Adventurous 26th president, he was 1st to ride...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>adventurous 26th president he was 1st to ride ...</td>\n",
       "      <td>theodore roosevelt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19301</th>\n",
       "      <td>10</td>\n",
       "      <td>1984-09-21</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>LABOR UNIONS</td>\n",
       "      <td>$200</td>\n",
       "      <td>Notorious labor leader missing since '75</td>\n",
       "      <td>Jimmy Hoffa</td>\n",
       "      <td>notorious labor leader missing since 75</td>\n",
       "      <td>jimmy hoffa</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19302</th>\n",
       "      <td>10</td>\n",
       "      <td>1984-09-21</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>1789</td>\n",
       "      <td>$200</td>\n",
       "      <td>Washington proclaimed Nov. 26, 1789 this first...</td>\n",
       "      <td>Thanksgiving</td>\n",
       "      <td>washington proclaimed nov 26 1789 this first n...</td>\n",
       "      <td>thanksgiving</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19303</th>\n",
       "      <td>10</td>\n",
       "      <td>1984-09-21</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>TOURIST TRAPS</td>\n",
       "      <td>$200</td>\n",
       "      <td>Both Ferde Grofe' &amp; the Colorado River dug thi...</td>\n",
       "      <td>the Grand Canyon</td>\n",
       "      <td>both ferde grofe  the colorado river dug this ...</td>\n",
       "      <td>the grand canyon</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19304</th>\n",
       "      <td>10</td>\n",
       "      <td>1984-09-21</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>LITERATURE</td>\n",
       "      <td>$200</td>\n",
       "      <td>Depending on the book, he could be a \"Jones\", ...</td>\n",
       "      <td>Tom</td>\n",
       "      <td>depending on the book he could be a jones a sa...</td>\n",
       "      <td>tom</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Show Number   Air Date             Round         Category Value  \\\n",
       "19325           10 1984-09-21   Final Jeopardy!  U.S. PRESIDENTS  None   \n",
       "19301           10 1984-09-21  Double Jeopardy!     LABOR UNIONS  $200   \n",
       "19302           10 1984-09-21  Double Jeopardy!             1789  $200   \n",
       "19303           10 1984-09-21  Double Jeopardy!    TOURIST TRAPS  $200   \n",
       "19304           10 1984-09-21  Double Jeopardy!       LITERATURE  $200   \n",
       "\n",
       "                                                Question              Answer  \\\n",
       "19325  Adventurous 26th president, he was 1st to ride...  Theodore Roosevelt   \n",
       "19301           Notorious labor leader missing since '75         Jimmy Hoffa   \n",
       "19302  Washington proclaimed Nov. 26, 1789 this first...        Thanksgiving   \n",
       "19303  Both Ferde Grofe' & the Colorado River dug thi...    the Grand Canyon   \n",
       "19304  Depending on the book, he could be a \"Jones\", ...                 Tom   \n",
       "\n",
       "                                          clean_question        clean_answer  \\\n",
       "19325  adventurous 26th president he was 1st to ride ...  theodore roosevelt   \n",
       "19301            notorious labor leader missing since 75         jimmy hoffa   \n",
       "19302  washington proclaimed nov 26 1789 this first n...        thanksgiving   \n",
       "19303  both ferde grofe  the colorado river dug this ...    the grand canyon   \n",
       "19304  depending on the book he could be a jones a sa...                 tom   \n",
       "\n",
       "       clean_value  answer_in_question  \n",
       "19325            0                 0.0  \n",
       "19301          200                 0.0  \n",
       "19302          200                 0.0  \n",
       "19303          200                 0.0  \n",
       "19304          200                 0.0  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_overlap = []\n",
    "terms_used = set([])\n",
    "\n",
    "jeopardy.sort_values(by = 'Air Date', inplace = True)\n",
    "\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over dataframe\n",
    "for index,series in jeopardy.iterrows():\n",
    "    # Splitting question\n",
    "    split_question = series['clean_question'].split(' ')\n",
    "    split_question = [x for x in split_question if len(x) >= 6]\n",
    "    \n",
    "    # Counter of matches for a specific row...\n",
    "    match_count = 0\n",
    "    for s in split_question:\n",
    "        if s in terms_used:\n",
    "            match_count += 1\n",
    "        terms_used.add(s)\n",
    "    \n",
    "    if len(split_question) > 0:\n",
    "        questions_overlap.append(match_count / len(split_question))\n",
    "    else:\n",
    "        questions_overlap.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894006357823148"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['question_overlap'] = pd.Series(questions_overlap)\n",
    "jeopardy['question_overlap'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are many questions that use the same terms as before... They are recycling a lot of questions OR they are basically asking about the same themes and things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low value vs High value questions\n",
    "\n",
    "Let's say you only want to study questions that pertain to high value questions instead of low value questions. This will help you earn more money when you're on Jeopardy.\n",
    "\n",
    "**You can actually figure out which terms correspond to high-value questions using a chi-squared test**. You'll first need to narrow down the questions into two categories:\n",
    "\n",
    "- **Low value** -- Any row where Value is less than 800.\n",
    "- **High value** -- Any row where Value is greater than 800.\n",
    "\n",
    "You'll then be able to loop through each of the terms from the last screen, `terms_used`, and:\n",
    "\n",
    "- Find the number of low value questions the word occurs in.\n",
    "- Find the number of high value questions the word occurs in.\n",
    "- Find the percentage of questions the word occurs in.\n",
    "- Based on the percentage of questions the word occurs in, find expected counts.\n",
    "- Compute the chi squared value based on the expected counts and the observed counts for high and low value questions.\n",
    "\n",
    "You can then find the words with the biggest differences in usage between high and low value questions, by selecting the words with the highest associated chi-squared values. Doing this for all of the words would take a very long time, so we'll just do it for a small sample now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['high_value'] = jeopardy.apply(lambda x: 1 if x['clean_value'] > 800 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_high_low(word):\n",
    "    \"Returns two numbers: the frequency with which the word appear in high- or low-value questions\"\n",
    "    high_count = low_count = 0\n",
    "\n",
    "    for index,series in jeopardy.iterrows():\n",
    "        split_question = series['clean_question'].split(' ')\n",
    "        if word in split_question:\n",
    "            if series['high_value'] == 1:\n",
    "                high_count += 1\n",
    "            else: \n",
    "                low_count += 1\n",
    "    \n",
    "    return high_count, low_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jakarta', 'tanganyika', 'sylphide', 'hrefhttpwwwjarchivecommedia20001123_dj_19wmvithese', 'intoned', 'consort', 'francesco', 'target', 'appearedin', 'gossip', '365bodypoint', 'junker', 'hrefhttpwwwjarchivecommedia20080313_dj_28jpg', 'anybody', 'brillante', 'restore', 'retsyn', 'currie', 'conestoga', 'josiah', 'raffaele', 'sugarless', 'dammini', 'welcomes', 'portion', 'father', 'leadeth', 'raping', 'grandparents', 'bonneville', 'lifeguard', 'sheryl', 'inmate', 'devourer', 'hrefhttpwwwjarchivecommedia20090105_dj_09jpg', 'flavian', 'highestranking', 'judean', 'hrefhttpwwwjarchivecommedia20091002_j_28jpg', 'vocabulary']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 3),\n",
       " (0, 1),\n",
       " (5, 2),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 4),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 2),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (2, 8),\n",
       " (26, 84),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 4),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "comparison_terms = random.choices(list(terms_used), k = 40)\n",
    "\n",
    "print(comparison_terms)\n",
    "\n",
    "observed_expected = list( map(word_high_low, comparison_terms) )\n",
    "\n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the chi-squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "high_value_counts = jeopardy['high_value'].sum()\n",
    "low_value_counts = jeopardy.shape[0] - high_value_counts\n",
    "\n",
    "chi_squared = []\n",
    "p_values = []\n",
    "\n",
    "for term in observed_expected:\n",
    "    total = term[0] + term[1]\n",
    "    total_prop = total / jeopardy.shape[0]\n",
    "    expected_high = total_prop * high_value_counts\n",
    "    expected_low = total_prop * low_value_counts\n",
    "\n",
    "    chi, p_value = chisquare(list(term), [expected_high, expected_low])\n",
    "    #print(chi, p_value)\n",
    "    chi_squared.append(chi)\n",
    "    p_values.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.4448774816612795,\n",
       "  0.401962846126884,\n",
       "  2.487792117195675,\n",
       "  0.401962846126884,\n",
       "  2.487792117195675,\n",
       "  1.205888538380652,\n",
       "  0.401962846126884,\n",
       "  6.2575220449142,\n",
       "  0.401962846126884,\n",
       "  0.803925692253768,\n",
       "  0.401962846126884,\n",
       "  2.487792117195675,\n",
       "  2.487792117195675,\n",
       "  0.401962846126884,\n",
       "  0.401962846126884,\n",
       "  1.607851384507536,\n",
       "  0.401962846126884,\n",
       "  2.487792117195675,\n",
       "  0.4448774816612795,\n",
       "  0.803925692253768,\n",
       "  2.487792117195675,\n",
       "  0.401962846126884,\n",
       "  2.487792117195675,\n",
       "  0.401962846126884,\n",
       "  0.36767906209032747,\n",
       "  1.3636119408688154,\n",
       "  0.803925692253768,\n",
       "  0.401962846126884,\n",
       "  0.803925692253768,\n",
       "  0.401962846126884,\n",
       "  0.803925692253768,\n",
       "  1.607851384507536,\n",
       "  0.401962846126884,\n",
       "  0.401962846126884,\n",
       "  0.401962846126884,\n",
       "  2.487792117195675,\n",
       "  0.401962846126884,\n",
       "  0.401962846126884,\n",
       "  0.401962846126884,\n",
       "  0.4448774816612795],\n",
       " [0.5047776487545996,\n",
       "  0.5260772985705469,\n",
       "  0.11473257634454047,\n",
       "  0.5260772985705469,\n",
       "  0.11473257634454047,\n",
       "  0.27214791766901714,\n",
       "  0.5260772985705469,\n",
       "  0.012366706058156086,\n",
       "  0.5260772985705469,\n",
       "  0.3699222378079571,\n",
       "  0.5260772985705469,\n",
       "  0.11473257634454047,\n",
       "  0.11473257634454047,\n",
       "  0.5260772985705469,\n",
       "  0.5260772985705469,\n",
       "  0.2047940943922556,\n",
       "  0.5260772985705469,\n",
       "  0.11473257634454047,\n",
       "  0.5047776487545996,\n",
       "  0.3699222378079571,\n",
       "  0.11473257634454047,\n",
       "  0.5260772985705469,\n",
       "  0.11473257634454047,\n",
       "  0.5260772985705469,\n",
       "  0.5442721040962595,\n",
       "  0.24291248030119686,\n",
       "  0.3699222378079571,\n",
       "  0.5260772985705469,\n",
       "  0.3699222378079571,\n",
       "  0.5260772985705469,\n",
       "  0.3699222378079571,\n",
       "  0.2047940943922556,\n",
       "  0.5260772985705469,\n",
       "  0.5260772985705469,\n",
       "  0.5260772985705469,\n",
       "  0.11473257634454047,\n",
       "  0.5260772985705469,\n",
       "  0.5260772985705469,\n",
       "  0.5260772985705469,\n",
       "  0.5047776487545996],\n",
       " [(1, 1),\n",
       "  (0, 1),\n",
       "  (1, 0),\n",
       "  (0, 1),\n",
       "  (1, 0),\n",
       "  (0, 3),\n",
       "  (0, 1),\n",
       "  (5, 2),\n",
       "  (0, 1),\n",
       "  (0, 2),\n",
       "  (0, 1),\n",
       "  (1, 0),\n",
       "  (1, 0),\n",
       "  (0, 1),\n",
       "  (0, 1),\n",
       "  (0, 4),\n",
       "  (0, 1),\n",
       "  (1, 0),\n",
       "  (1, 1),\n",
       "  (0, 2),\n",
       "  (1, 0),\n",
       "  (0, 1),\n",
       "  (1, 0),\n",
       "  (0, 1),\n",
       "  (2, 8),\n",
       "  (26, 84),\n",
       "  (0, 2),\n",
       "  (0, 1),\n",
       "  (0, 2),\n",
       "  (0, 1),\n",
       "  (0, 2),\n",
       "  (0, 4),\n",
       "  (0, 1),\n",
       "  (0, 1),\n",
       "  (0, 1),\n",
       "  (1, 0),\n",
       "  (0, 1),\n",
       "  (0, 1),\n",
       "  (0, 1),\n",
       "  (1, 1)],\n",
       " ['jakarta',\n",
       "  'tanganyika',\n",
       "  'sylphide',\n",
       "  'hrefhttpwwwjarchivecommedia20001123_dj_19wmvithese',\n",
       "  'intoned',\n",
       "  'consort',\n",
       "  'francesco',\n",
       "  'target',\n",
       "  'appearedin',\n",
       "  'gossip',\n",
       "  '365bodypoint',\n",
       "  'junker',\n",
       "  'hrefhttpwwwjarchivecommedia20080313_dj_28jpg',\n",
       "  'anybody',\n",
       "  'brillante',\n",
       "  'restore',\n",
       "  'retsyn',\n",
       "  'currie',\n",
       "  'conestoga',\n",
       "  'josiah',\n",
       "  'raffaele',\n",
       "  'sugarless',\n",
       "  'dammini',\n",
       "  'welcomes',\n",
       "  'portion',\n",
       "  'father',\n",
       "  'leadeth',\n",
       "  'raping',\n",
       "  'grandparents',\n",
       "  'bonneville',\n",
       "  'lifeguard',\n",
       "  'sheryl',\n",
       "  'inmate',\n",
       "  'devourer',\n",
       "  'hrefhttpwwwjarchivecommedia20090105_dj_09jpg',\n",
       "  'flavian',\n",
       "  'highestranking',\n",
       "  'judean',\n",
       "  'hrefhttpwwwjarchivecommedia20091002_j_28jpg',\n",
       "  'vocabulary'])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared, p_values, observed_expected, comparison_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.2575220449142, 0.012366706058156086, (5, 2), 'target')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 7\n",
    "chi_squared[i], p_values[i], observed_expected[i], comparison_terms[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
