{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying data\n",
    "\n",
    "Previously, we focused on the details around collecting data, on understanding its structure and how it's measured. Collecting data is just the starting point in a data analysis workflow. We rarely collect data just for the sake of collecting it. We collect data to analyze it, and we analyze it for different purposes:\n",
    "\n",
    "- To describe phenomena in the world (science).\n",
    "- To make better decisions (industries).\n",
    "- To improve systems (engineering).\n",
    "- To describe different aspects of our society (data journalism); etc.\n",
    "\n",
    "![img](steps.png)\n",
    "\n",
    "\n",
    "Our capacity to understand a data set just by looking at it in a table format is limited, and it decreases dramatically as the size of the data set increases. To be able to analyze data, we need to find ways to simplify it.\n",
    "\n",
    "The WNBA data set we've been working with has 143 rows and 32 columns. This might not seem like much compared to other data sets, but it's still extremely difficult to find any patterns just by eyeballing the data set in a table format. With 32 columns, even five rows would take us a couple of minutes to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Birth_Place</th>\n",
       "      <th>Birthdate</th>\n",
       "      <th>Age</th>\n",
       "      <th>College</th>\n",
       "      <th>...</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TO</th>\n",
       "      <th>PTS</th>\n",
       "      <th>DD2</th>\n",
       "      <th>TD3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aerial Powers</td>\n",
       "      <td>DAL</td>\n",
       "      <td>F</td>\n",
       "      <td>183</td>\n",
       "      <td>71.0</td>\n",
       "      <td>21.200991</td>\n",
       "      <td>US</td>\n",
       "      <td>January 17, 1994</td>\n",
       "      <td>23</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alana Beard</td>\n",
       "      <td>LA</td>\n",
       "      <td>G/F</td>\n",
       "      <td>185</td>\n",
       "      <td>73.0</td>\n",
       "      <td>21.329438</td>\n",
       "      <td>US</td>\n",
       "      <td>May 14, 1982</td>\n",
       "      <td>35</td>\n",
       "      <td>Duke</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>82</td>\n",
       "      <td>101</td>\n",
       "      <td>72</td>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alex Bentley</td>\n",
       "      <td>CON</td>\n",
       "      <td>G</td>\n",
       "      <td>170</td>\n",
       "      <td>69.0</td>\n",
       "      <td>23.875433</td>\n",
       "      <td>US</td>\n",
       "      <td>October 27, 1990</td>\n",
       "      <td>26</td>\n",
       "      <td>Penn State</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>78</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex Montgomery</td>\n",
       "      <td>SAN</td>\n",
       "      <td>G/F</td>\n",
       "      <td>185</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24.543462</td>\n",
       "      <td>US</td>\n",
       "      <td>December 11, 1988</td>\n",
       "      <td>28</td>\n",
       "      <td>Georgia Tech</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>134</td>\n",
       "      <td>169</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexis Jones</td>\n",
       "      <td>MIN</td>\n",
       "      <td>G</td>\n",
       "      <td>175</td>\n",
       "      <td>78.0</td>\n",
       "      <td>25.469388</td>\n",
       "      <td>US</td>\n",
       "      <td>August 5, 1994</td>\n",
       "      <td>23</td>\n",
       "      <td>Baylor</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Team  Pos  Height  Weight        BMI Birth_Place  \\\n",
       "0    Aerial Powers  DAL    F     183    71.0  21.200991          US   \n",
       "1      Alana Beard   LA  G/F     185    73.0  21.329438          US   \n",
       "2     Alex Bentley  CON    G     170    69.0  23.875433          US   \n",
       "3  Alex Montgomery  SAN  G/F     185    84.0  24.543462          US   \n",
       "4     Alexis Jones  MIN    G     175    78.0  25.469388          US   \n",
       "\n",
       "           Birthdate  Age         College  ... OREB  DREB  REB  AST  STL  BLK  \\\n",
       "0   January 17, 1994   23  Michigan State  ...    6    22   28   12    3    6   \n",
       "1       May 14, 1982   35            Duke  ...   19    82  101   72   63   13   \n",
       "2   October 27, 1990   26      Penn State  ...    4    36   40   78   22    3   \n",
       "3  December 11, 1988   28    Georgia Tech  ...   35   134  169   65   20   10   \n",
       "4     August 5, 1994   23          Baylor  ...    3     9   12   12    7    0   \n",
       "\n",
       "   TO  PTS  DD2  TD3  \n",
       "0  12   93    0    0  \n",
       "1  40  217    0    0  \n",
       "2  24  218    0    0  \n",
       "3  38  188    2    0  \n",
       "4  14   50    0    0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wnba = pd.read_csv('wnba.csv')\n",
    "\n",
    "wnba.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to simplify this data set is to select a variable, **count how many times each unique value occurs, and represent the frequencies (the number of times a unique value occurs) in a table**. This is how such a table looks for the Pos (player position) variable:\n",
    "\n",
    "![img](img/position.png)\n",
    "\n",
    "Because 60 of the players in our data set play as guards, the frequency for guards is 60. Because 33 of the players are forwards, the frequency for forwards is 33, and so on.\n",
    "\n",
    "With the table above, we simplified the Pos variable by transforming it to a **comprehensible format**. Instead of having to deal with analyzing 143 values (the length of the Pos variable), now we only have five values to analyze. We can make a few conclusions now that would have been difficult and time consuming to reach at just by looking at the list of 143 values:\n",
    "\n",
    "- We can see how the frequencies are distributed:\n",
    "    - Almost half of the players play as guards.\n",
    "    - Most of the players are either guards, forwards or centers.\n",
    "    - Very few players have combined positions (like guard/forward or forward/center).\n",
    "- We can make comparisons with ease:\n",
    "    - There are roughly two times more guards than forwards.\n",
    "    - There are slightly less centers that forwards; etc.\n",
    "    \n",
    "Because the table above shows how *frequencies are distributed*, it's often called a **frequency distribution table**, or, shorter, **frequency table** or **frequency distribution**. Throughout this mission, our focus will be on learning the details behind this form of simplifying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution tables\n",
    "\n",
    "A frequency distribution table has two columns. One column records the unique values of a variable, and the other the frequency of each unique value.\n",
    "\n",
    "![img](img/table.png)\n",
    "\n",
    "To generate a frequency distribution table using Python, we can use the Series.value_counts() method. Let's try it on the Pos column, which describes the position on the court of each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G      60\n",
       "F      33\n",
       "C      25\n",
       "G/F    13\n",
       "F/C    12\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Pos'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the Series.value_counts() method, generate frequency distribution tables for the Height column:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188    20\n",
       "193    18\n",
       "175    16\n",
       "185    15\n",
       "191    11\n",
       "183    11\n",
       "173    11\n",
       "196     9\n",
       "178     8\n",
       "180     7\n",
       "170     6\n",
       "198     5\n",
       "201     2\n",
       "168     2\n",
       "206     1\n",
       "165     1\n",
       "Name: Height, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Height'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting frequency distribution tables\n",
    "\n",
    "As you might have noticed, pandas sorts the tables by default in the descending order **of the frequencies**. Let's consider again the frequency distribution table for the Pos variable, which is measured on a nominal scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G      60\n",
       "F      33\n",
       "C      25\n",
       "G/F    13\n",
       "F/C    12\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Pos'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This default is harmless for variables measured on a nominal scale because the unique values, although different, have no direction (we can't say, for instance, that centers are greater or lower than guards). The default actually helps because we can immediately see which values have the greatest or lowest frequencies, we can make comparisons easily, etc.\n",
    "\n",
    "![img](img/vars_table.png)\n",
    "\n",
    "For variables measured on ordinal, interval, or ratio scales, this default makes the analysis of the tables more difficult because the unique values have direction (some uniques values are greater or lower than others). Let's consider the table for the Height variable, which is measured on a ratio scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188    20\n",
       "193    18\n",
       "175    16\n",
       "185    15\n",
       "191    11\n",
       "183    11\n",
       "173    11\n",
       "196     9\n",
       "178     8\n",
       "180     7\n",
       "170     6\n",
       "198     5\n",
       "201     2\n",
       "168     2\n",
       "206     1\n",
       "165     1\n",
       "Name: Height, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Height'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Height variable has direction, we might be interested to find:\n",
    "\n",
    "- How many players are under 170 cm?\n",
    "- How many players are very tall (over 185)?\n",
    "- Are there any players below 160 cm?\n",
    "\n",
    "It's time-consuming to answer these questions using the table above. The solution is to sort the table ourselves.\n",
    "\n",
    "wnba['Height'].value_counts() returns a Series object with the measures of height as indices. This allows us to sort the table by index using the Series.sort_index() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165     1\n",
       "168     2\n",
       "170     6\n",
       "173    11\n",
       "175    16\n",
       "178     8\n",
       "180     7\n",
       "183    11\n",
       "185    15\n",
       "188    20\n",
       "191    11\n",
       "193    18\n",
       "196     9\n",
       "198     5\n",
       "201     2\n",
       "206     1\n",
       "Name: Height, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Height'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort the table by index in a descending order using \n",
    "\n",
    "`wnba['Height'].value_counts().sort_index(ascending = False)`\n",
    "\n",
    "*Generate a frequency distribution table for the Age variable, which is measured on a ratio scale, and sort the table by unique values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21     2\n",
       "22    10\n",
       "23    15\n",
       "24    16\n",
       "25    15\n",
       "26    12\n",
       "27    13\n",
       "28    14\n",
       "29     8\n",
       "30     9\n",
       "31     8\n",
       "32     8\n",
       "33     3\n",
       "34     5\n",
       "35     4\n",
       "36     1\n",
       "Name: Age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "36     1\n",
       "35     4\n",
       "34     5\n",
       "33     3\n",
       "32     8\n",
       "31     8\n",
       "30     9\n",
       "29     8\n",
       "28    14\n",
       "27    13\n",
       "26    12\n",
       "25    15\n",
       "24    16\n",
       "23    15\n",
       "22    10\n",
       "21     2\n",
       "Name: Age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "age_ascending = wnba['Age'].value_counts().sort_index()\n",
    "display(age_ascending)\n",
    "age_descending = wnba['Age'].value_counts().sort_index(ascending = False)\n",
    "display(age_descending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting tables for ordinal variables\n",
    "\n",
    "The sorting techniques learned in the previous screen can't be used for ordinal scales where the measurement is done using words. We don't have a variable measured on an ordinal scale in our data set, but let's use the PTS variable and the conventions below to create one and see why the techniques we learned don't work:\n",
    "\n",
    "![img](img/cond.png)\n",
    "\n",
    "We name the new column PTS_ordinal_scale. Below is a short extract from our data set containing the new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pts_ordinal(row):\n",
    "    if row['PTS'] <= 20:\n",
    "        return 'very few points'\n",
    "    if (20 < row['PTS'] <=  80):\n",
    "        return 'few points'\n",
    "    if (80 < row['PTS'] <=  150):\n",
    "        return 'many, but below average'\n",
    "    if (150 < row['PTS'] <= 300):\n",
    "        return 'average number of points'\n",
    "    if (300 < row['PTS'] <=  450):\n",
    "        return 'more than average'\n",
    "    else:\n",
    "        return 'much more than average'\n",
    "    \n",
    "wnba['PTS_ordinal_scale'] = wnba.apply(make_pts_ordinal, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     many, but below average\n",
       "1    average number of points\n",
       "2    average number of points\n",
       "3    average number of points\n",
       "4                  few points\n",
       "Name: PTS_ordinal_scale, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS_ordinal_scale'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the frequency distribution table for the PTS_ordinal_scale variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average number of points    45\n",
       "few points                  27\n",
       "many, but below average     25\n",
       "more than average           21\n",
       "much more than average      13\n",
       "very few points             12\n",
       "Name: PTS_ordinal_scale, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS_ordinal_scale'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sort the labels in an ascending or descending order, but using Series.sort_index() doesn't work because the method can't infer quantities from words like \"few points\". Series.sort_index() can only order the index alphabetically in an ascending or descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average number of points    45\n",
       "few points                  27\n",
       "many, but below average     25\n",
       "more than average           21\n",
       "much more than average      13\n",
       "very few points             12\n",
       "Name: PTS_ordinal_scale, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS_ordinal_scale'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to do selection by index label. The output of `wnba['PTS_ordinal_scale'].value_counts()` is a Series object with the labels as indices. This means **we can select by indices to reorder in any way we like**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "very few points             12\n",
       "few points                  27\n",
       "many, but below average     25\n",
       "average number of points    45\n",
       "more than average           21\n",
       "much more than average      13\n",
       "Name: PTS_ordinal_scale, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS_ordinal_scale'].value_counts()[['very few points', 'few points', 'many, but below average', 'average number of points',\n",
    "                                          'more than average','much more than average']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be time-consuming because it involves **more typing than is ideal**. We can use iloc[] instead to reorder by position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "much more than average      13\n",
       "more than average           21\n",
       "average number of points    45\n",
       "many, but below average     25\n",
       "few points                  27\n",
       "very few points             12\n",
       "Name: PTS_ordinal_scale, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS_ordinal_scale'].value_counts().iloc[[4, 3, 0, 2, 1, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportions and percentages\n",
    "\n",
    "When we analyze distributions, we're often interested in answering questions about proportions and percentages. For instance, we may want to answer the following questions about the distribution of the Pos (player position) variable:\n",
    "\n",
    "- What proportion of players are guards?\n",
    "- What percentage of players are centers?\n",
    "- What percentage of players have mixed positions?\n",
    "\n",
    "It's very difficult to answer these questions precisely just by looking at the frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G      60\n",
       "F      33\n",
       "C      25\n",
       "G/F    13\n",
       "F/C    12\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Pos'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that almost half of the players are guards, but we need more granularity to answer the first question above. For that, we can transform frequencies to proportions.\n",
    "\n",
    "The proportion of each player position quantifies how many players play in a certain position relative to the total number of players. There are 60 guards and 143 players overall (including guards) so the proportion of guards is $\\frac{60}{143}$\n",
    "\n",
    "In practical data analysis, it's much more common to express the fraction as a decimal between 0 and 1. So we'd say that \n",
    "0.42 (the result of $\\frac{60}{143}$) of the players are guards.\n",
    "\n",
    "![img](img/proportion.png)\n",
    "\n",
    "In pandas, we can compute all the proportions at once by dividing each frequency by the total number of players:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G      0.419580\n",
       "F      0.230769\n",
       "C      0.174825\n",
       "G/F    0.090909\n",
       "F/C    0.083916\n",
       "Name: Pos, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Pos'].value_counts() / len(wnba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slightly faster though to use Series.value_counts() with the normalize parameter set to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G      0.419580\n",
       "F      0.230769\n",
       "C      0.174825\n",
       "G/F    0.090909\n",
       "F/C    0.083916\n",
       "Name: Pos, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Pos'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find percentages, we just have to multiply the proportions by 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G      41.958042\n",
       "F      23.076923\n",
       "C      17.482517\n",
       "G/F     9.090909\n",
       "F/C     8.391608\n",
       "Name: Pos, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Pos'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/perc.png)\n",
    "\n",
    "Because proportions and percentages are relative to the total number of instances in some set of data, they are called **relative frequencies**. In contrast, the frequencies we've been working with so far are called **absolute frequencies** because they are absolute counts and don't relate to the total number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions about the Age variable:\n",
    "\n",
    "- *What proportion of players are 25 years old?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1048951048951049"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_25 = wnba['Age'].value_counts(normalize = True)[25]\n",
    "proportion_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *What percentage of players are 30 years old?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.293706293706294"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_30 = wnba['Age'].value_counts(normalize = True)[30] * 100\n",
    "percentage_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *What percentage of players are 30 years or older?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.573426573426573"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_30 = wnba['Age'].value_counts(normalize = True).sort_index().loc[30:]\n",
    "percentage_over_30 = over_30.sum() * 100\n",
    "percentage_over_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *What percentage of players are 23 years or younger?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.881118881118883"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_23 = wnba['Age'].value_counts(normalize = True).sort_index(ascending = False).loc[23:]\n",
    "percentage_below_23 = below_23.sum() * 100\n",
    "percentage_below_23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentiles and percentile Ranks\n",
    "\n",
    "In the previous exercise, we found that the percentage of players aged 23 years or younger is 19% (rounded to the nearest integer). This percentage is also called a **percentile rank**.\n",
    "\n",
    "A percentile rank of a value $x$  in a frequency distribution is given by the percentage of values that are equal or less than \n",
    "$x$. In our last exercise, $x=23$, and the fact that 23 has a percentile rank of 19% means that 19% of the values are equal to or less than 23.\n",
    "\n",
    "In this context, the value of 23 is called the 19th percentile. If a value $x$ is the 19th percentile, it means that 19% of all the values in the distribution are equal to or less than $x$.\n",
    "\n",
    "![img](img/percentile.png)\n",
    "\n",
    "When we're trying to answer questions similar to *\"What percentage of players are 23 years or younger?\"*, **we're trying to find percentile ranks**. In our previous exercise, our answer to this question was 18.881%. We can arrive at the same answer a bit faster using the `percentileofscore(a, score, kind='weak')` function from `scipy.stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.88111888111888\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "print(percentileofscore(a = wnba['Age'], score = 23, kind = 'weak'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use `kind = 'weak'` to indicate that we want to find the percentage of values thar are equal to or less than the value we specify in the score parameter.\n",
    "\n",
    "Another question we had was what percentage of players are 30 years or older. We can answer this question too using percentile ranks. First we need to find **the percentage of values equal to or less than 29 years (the percentile rank of 29)**. The rest of the values must be 30 years or more.\n",
    "\n",
    "![img](img/per2.png)\n",
    "\n",
    "In our exercise the answer we found was 26.573%. This is what we get using the technique we've just learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.573426573426573\n"
     ]
    }
   ],
   "source": [
    "print(100 - percentileofscore(wnba['Age'], 29, kind = 'weak'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next screen, we'll learn how to find quickly any percentile using pandas. For now, let's practice percentile ranks more.\n",
    "\n",
    "Import `percentileofscore()` from `scipy.stats`, and then use it to answer the following questions:\n",
    "\n",
    "- What percentage of players played half the number of games or less in the 2016-2017 season (there are 34 games in the WNBAâ€™s regular season)? Use the Games Played column to find the data you need, and assign your answer to a variable named percentile_rank_half_less.\n",
    "- What percentage of players played more than half the number of games of the season 2016-2017? Assign your result to percentage_half_more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.083916083916083"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_rank_half_less = percentileofscore(wnba['Games Played'], 17, kind = 'weak')\n",
    "percentile_rank_half_less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.91608391608392"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_half_more = 100 - percentile_rank_half_less\n",
    "percentage_half_more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding percentiles with pandas\n",
    "\n",
    "To find percentiles, we can use the `Series.describe()` method, which returns by default the 25th, the 50th, and the 75th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    143.000000\n",
       "mean      27.076923\n",
       "std        3.679170\n",
       "min       21.000000\n",
       "25%       24.000000\n",
       "50%       27.000000\n",
       "75%       30.000000\n",
       "max       36.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not interested here in the first three rows of the output (count, mean, and standard deviation). We can use iloc[] to isolate just the output we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min    21.0\n",
       "25%    24.0\n",
       "50%    27.0\n",
       "75%    30.0\n",
       "max    36.0\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Age'].describe().iloc[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 25th, 50th, and 75th percentiles pandas returns by default are the scores that divide the distribution into four equal parts.\n",
    "\n",
    "![img](img/parts.png)\n",
    "\n",
    "The three percentiles that divide the distribution in four equal parts are also known as quartiles (from the Latin quartus which means four). There are three quartiles in the distribution of the Age variable:\n",
    "\n",
    "- The first quartile (also called *lower* quartile) is 24 (note that 24 is also the 25th percentile).\n",
    "- The second quartile (also called the *middle* quartile) is 27 (note that 27 is also the 50th percentile).\n",
    "- And the third quartile (also called the *upper* quartile) is 30 (note that 30 is also the 75th percentile).\n",
    "\n",
    "We may be interested to find the percentiles for percentages other than 25%, 50%, or 75%. For that, we can use the `percentiles` parameter of `Series.describe()`. This parameter requires us to pass the percentages we want as proportions between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      21.0\n",
       "10%      23.0\n",
       "15%      23.0\n",
       "33%      25.0\n",
       "50%      27.0\n",
       "59.2%    28.0\n",
       "85%      31.0\n",
       "90%      32.0\n",
       "max      36.0\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Age'].describe(percentiles = [.1, .15, .33, .5, .592, .85, .9]).iloc[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentiles don't have a single standard definition, so don't be surprised if you get very similar (but not identical) values if you use different functions (especially if the functions come from different libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use the Age variable along with Series.describe() to answer the following questions:*\n",
    "\n",
    "- *What's the upper quartile of the Age variable? Assign your answer to a variable named age_upper_quartile.*\n",
    "- *What's the middle quartile of the Age variable? Assign your answer to a variable named age_middle_quartile.*\n",
    "- *What's the 95th percentile of the Age variable? Assign your answer to a variable named age_95th_percentile.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min    21.0\n",
       "25%    24.0\n",
       "50%    27.0\n",
       "75%    30.0\n",
       "95%    34.0\n",
       "max    36.0\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Age'].describe(percentiles = [.25, .5, .75, .95]).iloc[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_age = wnba['Age'].describe(percentiles = [.25, .5, .75, .95])\n",
    "age_upper_quartile = perc_age.loc['75%']\n",
    "age_middle_quartile = perc_age.loc['50%']\n",
    "age_95th_percentile = perc_age.loc['95%']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped frequency distribution tables\n",
    "\n",
    "With frequency tables, we're trying to transform relatively large and incomprehensible amounts of data to a table format we can understand. However, not all frequency tables are straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.0      1\n",
       "57.0      1\n",
       "58.0      1\n",
       "59.0      2\n",
       "62.0      1\n",
       "63.0      3\n",
       "64.0      5\n",
       "65.0      4\n",
       "66.0      8\n",
       "67.0      1\n",
       "68.0      2\n",
       "69.0      2\n",
       "70.0      3\n",
       "71.0      2\n",
       "73.0      6\n",
       "74.0      4\n",
       "75.0      4\n",
       "76.0      4\n",
       "77.0     10\n",
       "78.0      5\n",
       "79.0      6\n",
       "80.0      3\n",
       "81.0      5\n",
       "82.0      4\n",
       "83.0      4\n",
       "84.0      9\n",
       "85.0      2\n",
       "86.0      7\n",
       "87.0      6\n",
       "88.0      6\n",
       "89.0      3\n",
       "90.0      2\n",
       "91.0      3\n",
       "93.0      3\n",
       "95.0      2\n",
       "96.0      2\n",
       "97.0      1\n",
       "104.0     2\n",
       "108.0     1\n",
       "113.0     2\n",
       "Name: Weight, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Weight'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of granularity in the table above, but for this reason it's not easy to find patterns. The table for the Weight variable is a relatively happy case - the frequency tables for variables like PTS, BMI, or MIN are even more daunting.\n",
    "\n",
    "If the variable is measured on an interval or ratio scale, a common solution to this problem is to group the values in equal intervals. For the Weight variable, the values range from 55 to 113 kg, which amounts to a difference of 58 kg. We can try to segment this 58 kg interval in ten smaller and equal intervals. This will result in ten intervals of 5.8 kg each:\n",
    "\n",
    "![img](img/interval.png)\n",
    "\n",
    "Fortunately, pandas can handle this process gracefully. We only need to make use of the bins parameter of the `bins` parameter of `Series.value_counts()`. We want ten equal intervals, so we need to specify `bins = 10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.941, 60.8]     5\n",
       "(60.8, 66.6]      21\n",
       "(66.6, 72.4]      10\n",
       "(72.4, 78.2]      33\n",
       "(78.2, 84.0]      31\n",
       "(84.0, 89.8]      24\n",
       "(89.8, 95.6]      10\n",
       "(95.6, 101.4]      3\n",
       "(101.4, 107.2]     2\n",
       "(107.2, 113.0]     3\n",
       "Name: Weight, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Weight'].value_counts(bins = 10).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.941, 60.8]    0.034965\n",
       "(60.8, 66.6]      0.146853\n",
       "(66.6, 72.4]      0.069930\n",
       "(72.4, 78.2]      0.230769\n",
       "(78.2, 84.0]      0.216783\n",
       "(84.0, 89.8]      0.167832\n",
       "(89.8, 95.6]      0.069930\n",
       "(95.6, 101.4]     0.020979\n",
       "(101.4, 107.2]    0.013986\n",
       "(107.2, 113.0]    0.020979\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Weight'].value_counts(bins = 10, normalize = True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(54.941, 60.8], (60.8, 66.6] or (107.2, 113.0] are number intervals. The ( character indicates that the starting point is not included, while the ] indicates that the endpoint is included. (54.941, 60.8] means that 54.941 isn't included in the interval, while 60.8 is. The interval (54.941, 60.8] contains all real numbers greater than 54.941 and less than or equal to 60.8.\n",
    "\n",
    "We can see above that there are 10 equal intervals, 5.8 each. The first interval, (54.941, 60.8] is confusing, and has to do with how pandas internals show the output. One way to understand this is to convert 54.941 to 1 decimal point, like all the other values are. Then the first interval becomes (54.9, 60.8]. 54.9 is not included, so you can think that the interval starts at the minimum value of the Weight variable, which is 55.\n",
    "\n",
    "Because we group values in a table to get a better sense of frequencies in the distribution, the table we generated above is also known as a grouped frequency distribution table. Each group (interval) in a **grouped frequency distribution table** is also known as a **class interval**. (107.2, 113.0], for instance, is a class interval.\n",
    "\n",
    "Using the grouped frequency distribution table we generated above for the Weight variable, we can find patterns easier in the distribution of values:\n",
    "\n",
    "- Most players weigh somewhere between 70 and 90 kg.\n",
    "- Very few players weigh over 100 kg.\n",
    "- Very few players weigh under 60 kg; etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Examine the frequency table for the PTS (total points) variable trying to find some patterns in the distribution of values. Then, generate a grouped frequency distribution table for the PTS variable with the following characteristics:*\n",
    "\n",
    "- *The table has 10 class intervals.*\n",
    "- *For each class interval, the table shows percentages instead of frequencies.*\n",
    "- *The class intervals are sorted in descending order.*\n",
    "\n",
    "*Assign the table to a variable named grouped_freq_table, then print it and try again to find some patterns in the distribution of values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96     3\n",
       "217    3\n",
       "116    2\n",
       "277    2\n",
       "87     2\n",
       "      ..\n",
       "159    1\n",
       "161    1\n",
       "278    1\n",
       "171    1\n",
       "514    1\n",
       "Name: PTS, Length: 123, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525.8, 584.0]     3.496503\n",
       "(467.6, 525.8]     2.797203\n",
       "(409.4, 467.6]     5.594406\n",
       "(351.2, 409.4]     6.993007\n",
       "(293.0, 351.2]     5.594406\n",
       "(234.8, 293.0]    11.888112\n",
       "(176.6, 234.8]    13.986014\n",
       "(118.4, 176.6]    11.888112\n",
       "(60.2, 118.4]     16.783217\n",
       "(1.417, 60.2]     20.979021\n",
       "Name: PTS, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS'].value_counts(bins = 10, normalize = True).sort_index(ascending = False) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Loss\n",
    "\n",
    "When we generate grouped frequency distribution tables, there's an inevitable information loss. Let's consider this table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.417, 60.2]     30\n",
       "(60.2, 118.4]     24\n",
       "(176.6, 234.8]    20\n",
       "(234.8, 293.0]    17\n",
       "(118.4, 176.6]    17\n",
       "(351.2, 409.4]    10\n",
       "(409.4, 467.6]     8\n",
       "(293.0, 351.2]     8\n",
       "(525.8, 584.0]     5\n",
       "(467.6, 525.8]     4\n",
       "Name: PTS, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS'].value_counts(bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first interval, we can see there are 30 players who scored between 2 and 60 points (2 is the minimum value in our data set, and points in basketball can only be integers). However, because we grouped the values, we lost more granular information like:\n",
    "\n",
    "- How many players, if any, scored exactly 50 points.\n",
    "- How many players scored under 10 points.\n",
    "- How many players scored between 20 and 30 points, etc.\n",
    "\n",
    "To get back this granular information, we can increase the number of class intervals. However, if we do that, we end up again with a table that's lengthy and very difficult to analyze.\n",
    "\n",
    "On the other side, if we decrease the number of class intervals, we lose even more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.417, 118.4]    54\n",
       "(118.4, 234.8]    37\n",
       "(234.8, 351.2]    25\n",
       "(351.2, 467.6]    18\n",
       "(467.6, 584.0]     9\n",
       "Name: PTS, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS'].value_counts(bins = 5).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 54 players that scored between 2 and 118 points. We can get this information from the first table above too, but there's some extra information there: among these 54 players, 30 scored between 2 and 60 points, and 24 scored between 61 and 118 points. We lost this information when we decreased the number of class intervals from 10 to 5.\n",
    "\n",
    "We can conclude there is a trade-off between the information in a table, and how comprehensible the table is.\n",
    "\n",
    "![img](img/info.png)\n",
    "\n",
    "\n",
    "When we increase the number of class intervals, we can get more information, but the table becomes harder to analyze. When we decrease the number of class intervals, we get a boost in comprehensibility, but the amount of information in the table decreases.\n",
    "\n",
    "As a rule of thumb, 10 is a good number of class intervals to choose because it offers a good balance between information and comprehensibility.\n",
    "\n",
    "![img](img/10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability for grouped frequency tables\n",
    "\n",
    "Pandas helps a lot when we need to explore quickly grouped frequency tables. However, the intervals pandas outputs are confusing at first sight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.417, 118.4]    54\n",
       "(118.4, 234.8]    37\n",
       "(234.8, 351.2]    25\n",
       "(351.2, 467.6]    18\n",
       "(467.6, 584.0]     9\n",
       "Name: PTS, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['PTS'].value_counts(bins = 5).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we'd have to publish the table above in a blog post or a scientific paper. The readers will have a hard time understanding the intervals we chose. They'll also be puzzled by the decimal numbers because points in basketball can only be integers.\n",
    "\n",
    "To fix this, we can define the intervals ourselves. For the table above, we can define six intervals of 100 points each, and then count how many values fit in each interval. We'd like to end with a table like this:\n",
    "\n",
    "![img](img/like.png)\n",
    "\n",
    "Next, we show one way to code the intervals. We start with creating the intervals using the `pd.interval_range()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalIndex([(0, 100], (100, 200], (200, 300], (300, 400], (400, 500], (500, 600]],\n",
      "              closed='right',\n",
      "              dtype='interval[int64]')\n"
     ]
    }
   ],
   "source": [
    "intervals = pd.interval_range(start = 0, end = 600, freq = 100)\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a new Series using the intervals as indices, and, for now, 0 as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100]      0\n",
      "(100, 200]    0\n",
      "(200, 300]    0\n",
      "(300, 400]    0\n",
      "(400, 500]    0\n",
      "(500, 600]    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "gr_freq_table = pd.Series([0,0,0,0,0,0], index = intervals)\n",
    "print(gr_freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we loop through the values of the PTS column, and for each value:\n",
    "\n",
    "- We loop through the intervals we defined previously, and for each interval:\n",
    "    - We check whether the current value from the PTS column belongs to that interval.\n",
    "    - If the value doesn't belong to an interval, we continue the inner loop over the intervals.\n",
    "    - If the value belongs to an interval:\n",
    "        - We update the counting for that interval in gr_freq_table by adding 1.\n",
    "        - We exit the inner loop over the intervals with break because a value can belong to one interval only, and it makes no sense to continue the loop (without using break, we'll get the same output but we'll do many redundant iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100]      49\n",
      "(100, 200]    28\n",
      "(200, 300]    32\n",
      "(300, 400]    17\n",
      "(400, 500]    10\n",
      "(500, 600]     7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for value in wnba['PTS']:\n",
    "    for interval in intervals:\n",
    "        if value in interval:\n",
    "            gr_freq_table.loc[interval] += 1\n",
    "            break\n",
    "print(gr_freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a quick check of our work. There are 143 players in the data set, so the frequencies should add up to 143:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "print(gr_freq_table.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're not restricted by the minimum and maximum values of a variable when we define intervals. The minimum number of points is 2, and the maximum is 584, but our intervals range from 1 to 600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the techniques above, generate a grouped frequency table for the PTS variable. The table should have the following characteristics:*\n",
    "\n",
    "- *There are 10 class intervals.*\n",
    "- *The first class interval starts at 0 (not included).*\n",
    "- *The last class interval ends at 600 (included).*\n",
    "- Each interval has a range of 60 points.*\n",
    "\n",
    "Assign the table to a variable named gr_freq_table_10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 60]       30\n",
       "(60, 120]     25\n",
       "(120, 180]    17\n",
       "(180, 240]    22\n",
       "(240, 300]    15\n",
       "(300, 360]     7\n",
       "(360, 420]    11\n",
       "(420, 480]     7\n",
       "(480, 540]     4\n",
       "(540, 600]     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals = pd.interval_range(start = 0, end = 600, freq = 60)\n",
    "gr_freq_table_10 = pd.Series([0] * 10, index = intervals)\n",
    "\n",
    "# Loop over values of the 'PTS' column...\n",
    "for value in wnba['PTS']:\n",
    "    for interval in intervals:\n",
    "        if value in interval:\n",
    "            gr_freq_table_10.loc[interval] += 1\n",
    "            break\n",
    "gr_freq_table_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency tables and continuous variables\n",
    "\n",
    "Remember from the previous mission that a height of 175 cm is just an interval bounded by the real limits of 174.5 cm (lower real limit) and 175.5 (upper real limit). When we build frequency tables for continuous variables, we need to take into account that the values are intervals.\n",
    "\n",
    "The height of 175 cm has a frequency of 16 in the distribution of the Height variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnba['Height'].value_counts()[175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't mean that there are 16 players that are all exactly 175 cm tall. It rather means that there are 16 players with a height that's somewhere between 174.5 cm and 175.5 cm.\n",
    "\n",
    "A similar reasoning applies when we read grouped frequency tables. If we had an interval of (180, 190] for a continuous variable, 180 and 190 are not the real limits. Instead, the real limits are given by the interval (179.5, 190.5], with 179.5 being the lower real limit of 180, and 190.5 the upper real limit of 190.\n",
    "\n",
    "Continuous variables affect as well the way we read percentiles. For instance, the 50th percentile (middle quartile) in the distribution of the Height variable is 185 cm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min    165.0\n",
      "25%    176.5\n",
      "50%    185.0\n",
      "75%    191.0\n",
      "max    206.0\n",
      "Name: Height, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Height'].describe().iloc[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 50% of the values are less than or equal to 185.5 cm (the upper limit of 185 cm), not equal to 185 cm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
